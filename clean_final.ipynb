{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "import holidays\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400125</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408305</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 counter_id                       counter_name    site_id  \\\n",
       "400125  100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "408305  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "\n",
       "                            site_name                date  \\\n",
       "400125  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "408305  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "\n",
       "       counter_installation_date         coordinates counter_technical_id  \\\n",
       "400125                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "408305                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "\n",
       "         latitude  longitude  \n",
       "400125  48.840801   2.333233  \n",
       "408305  48.840801   2.333233  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "X, y = utils.get_train_data()\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dates(X):\n",
    "    X = X.copy()  # Modify a copy of X\n",
    "    \n",
    "    # Ensure 'date' is in datetime format\n",
    "    X[\"date\"] = pd.to_datetime(X[\"date\"])\n",
    "    \n",
    "    # Extract date components\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Identify weekends (Saturday = 5, Sunday = 6)\n",
    "    X[\"is_weekend\"] = X[\"weekday\"].isin([5, 6])\n",
    "    \n",
    "    # Get French holidays for all years in the dataset\n",
    "    years = X[\"year\"].unique()\n",
    "    fr_holidays = holidays.France(years=years)\n",
    "    \n",
    "    # Identify holidays\n",
    "    X[\"is_holiday\"] = X[\"date\"].dt.date.isin(fr_holidays)\n",
    "    \n",
    "    # Drop the original 'date' column\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400125</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408305</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 counter_id                       counter_name    site_id  \\\n",
       "400125  100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "408305  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "\n",
       "                            site_name                date  \\\n",
       "400125  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "408305  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "\n",
       "       counter_installation_date         coordinates counter_technical_id  \\\n",
       "400125                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "408305                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "\n",
       "         latitude  longitude  year  month  day  weekday  hour  is_weekend  \\\n",
       "400125  48.840801   2.333233  2020      9    1        1     1       False   \n",
       "408305  48.840801   2.333233  2020      9    1        1     1       False   \n",
       "\n",
       "        is_holiday  \n",
       "400125       False  \n",
       "408305       False  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_encoder = FunctionTransformer(_encode_dates, validate=False)\n",
    "X = date_encoder.fit_transform(X)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date: 2020-09-01 01:00:00\n",
      "Latest date: 2021-09-09 23:00:00\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'date' column is in datetime format\n",
    "X['date'] = pd.to_datetime(X['date'])\n",
    "\n",
    "# Find the earliest and latest dates\n",
    "earliest_date = X['date'].min()\n",
    "latest_date = X['date'].max()\n",
    "\n",
    "print(f\"Earliest date: {earliest_date}\")\n",
    "print(f\"Latest date: {latest_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "strike_data = {'date': [datetime(2023, 2, 7), datetime(2023, 2, 16), datetime(2023, 3, 7),\n",
    "                 datetime(2023, 1, 31), datetime(2022, 2, 18), datetime(2022, 3, 25),\n",
    "                 datetime(2022, 5, 23), datetime(2022, 9, 29), datetime(2022, 10, 13)],\n",
    "                'Strike': [1] * 9}\n",
    "\n",
    "# Create a DataFrame\n",
    "strike = pd.DataFrame(strike_data)\n",
    "\n",
    "# Sort the values by ascending date\n",
    "strike.sort_values(by='date', inplace=True)\n",
    "strike.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Merge the strike DataFrame with df\n",
    "X = X.merge(strike, on='date', how='left')\n",
    "X['Strike'] = X['Strike'].fillna(0).astype(int)\n",
    "\n",
    "#Create get_TimeOfDay_name and get_TimeOfDay functions \n",
    "def get_TimeOfDay_name(hour):\n",
    "  \n",
    "  if hour > 3 and hour <= 6:\n",
    "    return 'Early morning 4:00AM - 6:00 AM'  \n",
    "  if hour > 6 and hour <= 10:\n",
    "    return 'Morning 7:00AM - 10:00 AM'\n",
    "  elif hour > 10 and hour <= 13:\n",
    "    return 'Middle of the day 11:00 AM - 1:00 PM'\n",
    "  elif hour > 13 and hour <= 17:\n",
    "    return 'Afternoon 2:00 PM - 5:00 PM'\n",
    "  elif hour > 17 and hour <= 22:\n",
    "    return 'Evening 6:00 PM - 10:00 PM'\n",
    "  else :\n",
    "    return 'Night 11:00 PM - 3:00 AM'\n",
    "  \n",
    "def get_TimeOfDay(hour):\n",
    "  if hour > 3 and hour <= 6:\n",
    "    return 1  \n",
    "  if hour > 6 and hour <= 10:\n",
    "    return 2\n",
    "  elif hour > 10 and hour <= 13:\n",
    "    return 3\n",
    "  elif hour > 13 and hour <= 17:\n",
    "    return 4\n",
    "  elif hour > 17 and hour <= 22:\n",
    "    return 5\n",
    "  else :\n",
    "    return 6\n",
    "\n",
    "#Create columns by applying the functions\n",
    "X['TimeOfDay'] = X['hour'].apply(get_TimeOfDay)\n",
    "X['TimeOfDay_name'] = X['hour'].apply(get_TimeOfDay_name)\n",
    "\n",
    "def get_season_name(date):\n",
    "  if (date > datetime(2022, 3, 20) ) & (date < datetime(2022, 6, 21)):\n",
    "    return 'Spring'\n",
    "  if (date > datetime(2022, 6, 20)) & (date < datetime(2022, 9, 21)):\n",
    "    return 'Summer'\n",
    "  if (date > datetime(2022, 9, 20)) & (date < datetime(2022, 12, 21)):\n",
    "      return 'Fall'\n",
    "  if  ((date > datetime(2022, 12, 20)) & (date < datetime(2023, 3, 20))) | ((date > datetime(2021, 12, 31)) & (date < datetime(2022, 3, 21))):\n",
    "      return 'Winter'\n",
    "\n",
    "def get_season(date):\n",
    "  if (date > datetime(2022, 3, 20) ) & (date < datetime(2022, 6, 21)):\n",
    "    return 1\n",
    "  if (date > datetime(2022, 6, 20)) & (date < datetime(2022, 9, 21)):\n",
    "    return 2\n",
    "  if (date > datetime(2022, 9, 20)) & (date < datetime(2022, 12, 21)):\n",
    "    return 3\n",
    "  if  ((date > datetime(2022, 12, 20)) & (date < datetime(2023, 3, 20))) | ((date > datetime(2021, 12, 31)) & (date < datetime(2022, 3, 21))):\n",
    "    return 4\n",
    "\n",
    "#Create columns by applying the functions\n",
    "X['Season'] = X['date'].apply(get_season)\n",
    "X['Season_name'] = X['date'].apply(get_season_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.get_dummies(X, columns=[\"hour\"], prefix=\"hour\")\n",
    "# X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_temporal(X, y, delta_threshold=\"30 days\"):\n",
    "    \n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = (X[\"date\"] <= cutoff_date)\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with datetime64[ns] dtype: ['feature_0', 'feature_1', 'date']\n",
      "Columns with datetime64[ns] dtype: ['feature_0', 'feature_1', 'date']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocessing\n",
    "# One-hot encode the categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "categorical_encoded = onehot_encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Numerical scaling\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "numerical_scaled = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Drop processed columns\n",
    "X.drop(categorical_cols, axis=1, inplace=True)\n",
    "X.drop(numerical_cols, axis=1, inplace=True)\n",
    "\n",
    "# Ensure date column is in datetime format\n",
    "X[\"date\"] = pd.to_datetime(X[\"date\"])\n",
    "\n",
    "# Combine all features\n",
    "X_combined = np.hstack([X.values, categorical_encoded, numerical_scaled])\n",
    "\n",
    "# Assuming each sample has a single timestep\n",
    "\n",
    "# Step 3: Temporal Train-Test Split\n",
    "# Convert X_reshaped back into a DataFrame to preserve the date column\n",
    "X_combined_df = pd.DataFrame(X_combined, columns=[f\"feature_{i}\" for i in range(X_combined.shape[1])])\n",
    "X_combined_df[\"date\"] = X[\"date\"].values  # Restore the date column\n",
    "\n",
    "\n",
    "# Apply temporal train-test split\n",
    "X_train_split, y_train_split, X_test_split, y_test_split = train_test_split_temporal(X_combined_df, y)\n",
    "\n",
    "\n",
    "# Remove the 'date' column after splitting\n",
    "datetime_columns = X_test_split.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(f\"Columns with datetime64[ns] dtype: {datetime_columns.tolist()}\")\n",
    "datetime_columns = X_test_split.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(f\"Columns with datetime64[ns] dtype: {datetime_columns.tolist()}\")\n",
    "\n",
    "# Drop these columns from X_train_split\n",
    "X_train_split = X_train_split.drop(columns=datetime_columns)\n",
    "# Drop these columns from X_test_split\n",
    "X_test_split = X_test_split.drop(columns=datetime_columns)\n",
    "\n",
    "X_train_split = X_train_split.astype(float)\n",
    "X_test_split = X_test_split.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with datetime64[ns] dtype: ['feature_0', 'feature_1', 'date']\n"
     ]
    }
   ],
   "source": [
    "final_test = utils.get_test_data()\n",
    "date_encoder = FunctionTransformer(_encode_dates, validate=False)\n",
    "final_test = date_encoder.fit_transform(final_test)\n",
    "# final_test = pd.get_dummies(final_test, columns=[\"hour\"], prefix=\"hour\")\n",
    "# final_test.head(2)\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "# One-hot encode the categorical variables\n",
    "\n",
    "final_test = final_test.merge(strike, on='date', how='left')\n",
    "final_test['Strike'] = final_test['Strike'].fillna(0).astype(int)\n",
    "\n",
    "final_test['TimeOfDay'] = final_test['hour'].apply(get_TimeOfDay)\n",
    "final_test['TimeOfDay_name'] = final_test['hour'].apply(get_TimeOfDay_name)\n",
    "\n",
    "final_test['Season'] = final_test['date'].apply(get_season)\n",
    "final_test['Season_name'] = final_test['date'].apply(get_season_name)\n",
    "\n",
    "categorical_cols = final_test.select_dtypes(include=['object', 'category']).columns\n",
    "onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "categorical_encoded = onehot_encoder.fit_transform(final_test[categorical_cols])\n",
    "\n",
    "# Numerical scaling\n",
    "numerical_cols = final_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "numerical_scaled = scaler.fit_transform(final_test[numerical_cols])\n",
    "\n",
    "# Drop processed columns\n",
    "final_test.drop(categorical_cols, axis=1, inplace=True)\n",
    "final_test.drop(numerical_cols, axis=1, inplace=True)\n",
    "\n",
    "# Ensure date column is in datetime format\n",
    "final_test[\"date\"] = pd.to_datetime(final_test[\"date\"])\n",
    "\n",
    "# Combine all features\n",
    "final_test_combined = np.hstack([final_test.values, categorical_encoded, numerical_scaled])\n",
    "\n",
    "# Assuming each sample has a single timestep\n",
    "\n",
    "# Convert X_reshaped back into a DataFrame to preserve the date column\n",
    "final_test_combined = pd.DataFrame(final_test_combined, columns=[f\"feature_{i}\" for i in range(final_test_combined.shape[1])])\n",
    "final_test_combined[\"date\"] = final_test[\"date\"].values  # Restore the date column\n",
    "\n",
    "\n",
    "datetime_columns = final_test_combined.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(f\"Columns with datetime64[ns] dtype: {datetime_columns.tolist()}\")\n",
    "\n",
    "final_test_combined = final_test_combined.drop(columns=datetime_columns)\n",
    "\n",
    "final_test_combined = final_test_combined.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBoost, RMSE: 0.5120635104018924\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 456507, number of used features: 212\n",
      "[LightGBM] [Info] Start training from score 3.048868\n",
      "Model: LightGBM, RMSE: 0.5152787320057891\n",
      "              RMSE\n",
      "XGBoost   0.512064\n",
      "LightGBM  0.515279\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213', 'feature_214'] ['feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213']\nexpected feature_214 in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_df)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 37\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_test_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_test\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_bike_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: predictions\u001b[38;5;241m.\u001b[39mflatten()})\n\u001b[0;32m     39\u001b[0m     submission_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\xgboost\\sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\xgboost\\core.py:2514\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2512\u001b[0m     data, fns, _ \u001b[38;5;241m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[0;32m   2513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[1;32m-> 2514\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[0;32m   2516\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\xgboost\\core.py:3079\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[0;32m   3074\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   3075\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3076\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[0;32m   3077\u001b[0m     )\n\u001b[1;32m-> 3079\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names mismatch: ['feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213', 'feature_214'] ['feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27', 'feature_28', 'feature_29', 'feature_30', 'feature_31', 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_37', 'feature_38', 'feature_39', 'feature_40', 'feature_41', 'feature_42', 'feature_43', 'feature_44', 'feature_45', 'feature_46', 'feature_47', 'feature_48', 'feature_49', 'feature_50', 'feature_51', 'feature_52', 'feature_53', 'feature_54', 'feature_55', 'feature_56', 'feature_57', 'feature_58', 'feature_59', 'feature_60', 'feature_61', 'feature_62', 'feature_63', 'feature_64', 'feature_65', 'feature_66', 'feature_67', 'feature_68', 'feature_69', 'feature_70', 'feature_71', 'feature_72', 'feature_73', 'feature_74', 'feature_75', 'feature_76', 'feature_77', 'feature_78', 'feature_79', 'feature_80', 'feature_81', 'feature_82', 'feature_83', 'feature_84', 'feature_85', 'feature_86', 'feature_87', 'feature_88', 'feature_89', 'feature_90', 'feature_91', 'feature_92', 'feature_93', 'feature_94', 'feature_95', 'feature_96', 'feature_97', 'feature_98', 'feature_99', 'feature_100', 'feature_101', 'feature_102', 'feature_103', 'feature_104', 'feature_105', 'feature_106', 'feature_107', 'feature_108', 'feature_109', 'feature_110', 'feature_111', 'feature_112', 'feature_113', 'feature_114', 'feature_115', 'feature_116', 'feature_117', 'feature_118', 'feature_119', 'feature_120', 'feature_121', 'feature_122', 'feature_123', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_130', 'feature_131', 'feature_132', 'feature_133', 'feature_134', 'feature_135', 'feature_136', 'feature_137', 'feature_138', 'feature_139', 'feature_140', 'feature_141', 'feature_142', 'feature_143', 'feature_144', 'feature_145', 'feature_146', 'feature_147', 'feature_148', 'feature_149', 'feature_150', 'feature_151', 'feature_152', 'feature_153', 'feature_154', 'feature_155', 'feature_156', 'feature_157', 'feature_158', 'feature_159', 'feature_160', 'feature_161', 'feature_162', 'feature_163', 'feature_164', 'feature_165', 'feature_166', 'feature_167', 'feature_168', 'feature_169', 'feature_170', 'feature_171', 'feature_172', 'feature_173', 'feature_174', 'feature_175', 'feature_176', 'feature_177', 'feature_178', 'feature_179', 'feature_180', 'feature_181', 'feature_182', 'feature_183', 'feature_184', 'feature_185', 'feature_186', 'feature_187', 'feature_188', 'feature_189', 'feature_190', 'feature_191', 'feature_192', 'feature_193', 'feature_194', 'feature_195', 'feature_196', 'feature_197', 'feature_198', 'feature_199', 'feature_200', 'feature_201', 'feature_202', 'feature_203', 'feature_204', 'feature_205', 'feature_206', 'feature_207', 'feature_208', 'feature_209', 'feature_210', 'feature_211', 'feature_212', 'feature_213']\nexpected feature_214 in input data"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=42, verbosity=1),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=42),\n",
    "    # \"Random Forest\": RandomForestRegressor(\n",
    "    #     n_estimators=100,  # Fewer trees\n",
    "    #     max_depth=20,     # Limit depth\n",
    "    #     min_samples_split=5,\n",
    "    #     min_samples_leaf=2,\n",
    "    #     random_state=42,\n",
    "    #     n_jobs=-1         # Utilize multiple cores\n",
    "    # ),\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_split)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_split, y_pred))\n",
    "    results[name] = rmse\n",
    "    print(f\"Model: {name}, RMSE: {rmse}\")\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['RMSE']).sort_values(by='RMSE')\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "for name, model in models.items():\n",
    "    predictions = model.predict(final_test_combined)\n",
    "    submission = pd.DataFrame({\"id\": final_test.index, \"log_bike_count\": predictions.flatten()})\n",
    "    submission_path = f\"submission_{name}.csv\"\n",
    "    submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 456507, number of used features: 212\n",
      "[LightGBM] [Info] Start training from score 3.048868\n",
      "RMSE of Stacked Model: 0.41133587592411264\n"
     ]
    }
   ],
   "source": [
    "# Train base models\n",
    "rf =  RandomForestRegressor(\n",
    "        n_estimators=100,  # Fewer trees\n",
    "        max_depth=20,     # Limit depth\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1         # Utilize multiple cores\n",
    "    )\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "rf.fit(X_train_split, y_train_split)\n",
    "# xgb_model.fit(X_train_split, y_train_split)\n",
    "lgb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Generate predictions for stacking\n",
    "rf_pred = rf.predict(X_test_split)\n",
    "# xgb_pred = xgb_model.predict(X_test_split)\n",
    "lgb_pred = lgb_model.predict(X_test_split)\n",
    "\n",
    "# Combine predictions as input to the meta-model\n",
    "stacked_features = np.vstack((rf_pred, lgb_pred)).T\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = xgb.XGBRegressor(random_state=42)\n",
    "meta_model.fit(stacked_features, y_test_split)\n",
    "\n",
    "# Final predictions\n",
    "final_pred = meta_model.predict(stacked_features)\n",
    "\n",
    "# Evaluate the stacked model\n",
    "rmse = np.sqrt(mean_squared_error(y_test_split, final_pred))\n",
    "print(f\"RMSE of Stacked Model: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = rf.predict(final_test_combined)\n",
    "lgb_pred = lgb_model.predict(final_test_combined)\n",
    "\n",
    "# Combine predictions as input to the meta-model\n",
    "stacked_features = np.vstack((rf_pred, lgb_pred)).T\n",
    "\n",
    "# Final predictions\n",
    "predictions = meta_model.predict(stacked_features)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": final_test.index, \"log_bike_count\": predictions.flatten()})\n",
    "submission_path = \"submission_meta_model.csv\"\n",
    "submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
