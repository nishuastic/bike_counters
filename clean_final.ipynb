{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import holidays\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400125</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408305</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 counter_id                       counter_name    site_id  \\\n",
       "400125  100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "408305  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "\n",
       "                            site_name                date  \\\n",
       "400125  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "408305  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "\n",
       "       counter_installation_date         coordinates counter_technical_id  \\\n",
       "400125                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "408305                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "\n",
       "         latitude  longitude  \n",
       "400125  48.840801   2.333233  \n",
       "408305  48.840801   2.333233  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "X, y = utils.get_train_data()\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_dates(X):\n",
    "    X = X.copy()  # Modify a copy of X\n",
    "    \n",
    "    # Ensure 'date' is in datetime format\n",
    "    X[\"date\"] = pd.to_datetime(X[\"date\"])\n",
    "    \n",
    "    # Extract date components\n",
    "    X[\"year\"] = X[\"date\"].dt.year\n",
    "    X[\"month\"] = X[\"date\"].dt.month\n",
    "    X[\"day\"] = X[\"date\"].dt.day\n",
    "    X[\"weekday\"] = X[\"date\"].dt.weekday\n",
    "    X[\"hour\"] = X[\"date\"].dt.hour\n",
    "\n",
    "    # Identify weekends (Saturday = 5, Sunday = 6)\n",
    "    X[\"is_weekend\"] = X[\"weekday\"].isin([5, 6])\n",
    "    \n",
    "    # Get French holidays for all years in the dataset\n",
    "    years = X[\"year\"].unique()\n",
    "    fr_holidays = holidays.France(years=years)\n",
    "    \n",
    "    # Identify holidays\n",
    "    X[\"is_holiday\"] = X[\"date\"].dt.date.isin(fr_holidays)\n",
    "    \n",
    "    # Drop the original 'date' column\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400125</th>\n",
       "      <td>100049407-353255860</td>\n",
       "      <td>152 boulevard du Montparnasse E-O</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408305</th>\n",
       "      <td>100049407-353255859</td>\n",
       "      <td>152 boulevard du Montparnasse O-E</td>\n",
       "      <td>100049407</td>\n",
       "      <td>152 boulevard du Montparnasse</td>\n",
       "      <td>2020-09-01 01:00:00</td>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>48.840801,2.333233</td>\n",
       "      <td>Y2H19070373</td>\n",
       "      <td>48.840801</td>\n",
       "      <td>2.333233</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 counter_id                       counter_name    site_id  \\\n",
       "400125  100049407-353255860  152 boulevard du Montparnasse E-O  100049407   \n",
       "408305  100049407-353255859  152 boulevard du Montparnasse O-E  100049407   \n",
       "\n",
       "                            site_name                date  \\\n",
       "400125  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "408305  152 boulevard du Montparnasse 2020-09-01 01:00:00   \n",
       "\n",
       "       counter_installation_date         coordinates counter_technical_id  \\\n",
       "400125                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "408305                2018-12-07  48.840801,2.333233          Y2H19070373   \n",
       "\n",
       "         latitude  longitude  year  month  day  weekday  hour  is_weekend  \\\n",
       "400125  48.840801   2.333233  2020      9    1        1     1       False   \n",
       "408305  48.840801   2.333233  2020      9    1        1     1       False   \n",
       "\n",
       "        is_holiday  \n",
       "400125       False  \n",
       "408305       False  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_encoder = FunctionTransformer(_encode_dates, validate=False)\n",
    "X = date_encoder.fit_transform(X)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.get_dummies(X, columns=[\"hour\"], prefix=\"hour\")\n",
    "# X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_temporal(X, y, delta_threshold=\"30 days\"):\n",
    "    \n",
    "    cutoff_date = X[\"date\"].max() - pd.Timedelta(delta_threshold)\n",
    "    mask = (X[\"date\"] <= cutoff_date)\n",
    "    X_train, X_valid = X.loc[mask], X.loc[~mask]\n",
    "    y_train, y_valid = y[mask], y[~mask]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with datetime64[ns] dtype: ['feature_0', 'feature_1', 'date']\n",
      "Columns with datetime64[ns] dtype: ['feature_0', 'feature_1', 'date']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "# One-hot encode the categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "categorical_encoded = onehot_encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Numerical scaling\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "numerical_scaled = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Drop processed columns\n",
    "X.drop(categorical_cols, axis=1, inplace=True)\n",
    "X.drop(numerical_cols, axis=1, inplace=True)\n",
    "\n",
    "# Ensure date column is in datetime format\n",
    "X[\"date\"] = pd.to_datetime(X[\"date\"])\n",
    "\n",
    "# Combine all features\n",
    "X_combined = np.hstack([X.values, categorical_encoded, numerical_scaled])\n",
    "\n",
    "# Step 2: Reshape for LSTM\n",
    "# LSTM requires 3D input: (samples, timesteps, features)\n",
    "# Assuming each sample has a single timestep\n",
    "X_reshaped = X_combined.reshape(X_combined.shape[0], 1, X_combined.shape[1])\n",
    "\n",
    "# Step 3: Temporal Train-Test Split\n",
    "# Convert X_reshaped back into a DataFrame to preserve the date column\n",
    "X_combined_df = pd.DataFrame(X_combined, columns=[f\"feature_{i}\" for i in range(X_combined.shape[1])])\n",
    "X_combined_df[\"date\"] = X[\"date\"].values  # Restore the date column\n",
    "\n",
    "\n",
    "# Apply temporal train-test split\n",
    "X_train_split, y_train_split, X_test_split, y_test_split = train_test_split_temporal(X_combined_df, y)\n",
    "\n",
    "\n",
    "# Remove the 'date' column after splitting\n",
    "datetime_columns = X_test_split.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(f\"Columns with datetime64[ns] dtype: {datetime_columns.tolist()}\")\n",
    "datetime_columns = X_test_split.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(f\"Columns with datetime64[ns] dtype: {datetime_columns.tolist()}\")\n",
    "\n",
    "# Drop these columns from X_train_split\n",
    "X_train_split = X_train_split.drop(columns=datetime_columns)\n",
    "# Drop these columns from X_test_split\n",
    "X_test_split = X_test_split.drop(columns=datetime_columns)\n",
    "\n",
    "X_train_split = X_train_split.astype(float)\n",
    "X_test_split = X_test_split.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = utils.get_test_data()\n",
    "date_encoder = FunctionTransformer(_encode_dates, validate=False)\n",
    "final_test = date_encoder.fit_transform(final_test)\n",
    "# final_test = pd.get_dummies(final_test, columns=[\"hour\"], prefix=\"hour\")\n",
    "# final_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Elastic Net, RMSE: 1.4149543225433936\n",
      "Model: XGBoost, RMSE: 0.5120635104018924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 456507, number of used features: 212\n",
      "[LightGBM] [Info] Start training from score 3.048868\n",
      "Model: LightGBM, RMSE: 0.5152787320057891\n",
      "Model: Random Forest, RMSE: 0.7521767458309048\n",
      "                   RMSE\n",
      "XGBoost        0.512064\n",
      "LightGBM       0.515279\n",
      "Random Forest  0.752177\n",
      "Elastic Net    1.414954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    # \"Elastic Net\": ElasticNet(random_state=42),\n",
    "    # \"XGBoost\": xgb.XGBRegressor(random_state=42, verbosity=1),\n",
    "    # \"LightGBM\": lgb.LGBMRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=100,  # Fewer trees\n",
    "        max_depth=20,     # Limit depth\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1         # Utilize multiple cores\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_split, y_train_split)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_split)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_split, y_pred))\n",
    "    results[name] = rmse\n",
    "    print(f\"Model: {name}, RMSE: {rmse}\")\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['RMSE']).sort_values(by='RMSE')\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with datetime64[ns] dtype: ['feature_0', 'feature_1', 'date']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "# One-hot encode the categorical variables\n",
    "categorical_cols = final_test.select_dtypes(include=['object', 'category']).columns\n",
    "onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "categorical_encoded = onehot_encoder.fit_transform(final_test[categorical_cols])\n",
    "\n",
    "# Numerical scaling\n",
    "numerical_cols = final_test.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "numerical_scaled = scaler.fit_transform(final_test[numerical_cols])\n",
    "\n",
    "# Drop processed columns\n",
    "final_test.drop(categorical_cols, axis=1, inplace=True)\n",
    "final_test.drop(numerical_cols, axis=1, inplace=True)\n",
    "\n",
    "# Ensure date column is in datetime format\n",
    "final_test[\"date\"] = pd.to_datetime(final_test[\"date\"])\n",
    "\n",
    "# Combine all features\n",
    "X_combined = np.hstack([final_test.values, categorical_encoded, numerical_scaled])\n",
    "\n",
    "# Step 2: Reshape for LSTM\n",
    "# LSTM requires 3D input: (samples, timesteps, features)\n",
    "# Assuming each sample has a single timestep\n",
    "X_reshaped = X_combined.reshape(X_combined.shape[0], 1, X_combined.shape[1])\n",
    "\n",
    "# Step 3: Temporal Train-Test Split\n",
    "# Convert X_reshaped back into a DataFrame to preserve the date column\n",
    "X_combined_df = pd.DataFrame(X_combined, columns=[f\"feature_{i}\" for i in range(X_combined.shape[1])])\n",
    "X_combined_df[\"date\"] = final_test[\"date\"].values  # Restore the date column\n",
    "\n",
    "\n",
    "datetime_columns = X_combined_df.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(f\"Columns with datetime64[ns] dtype: {datetime_columns.tolist()}\")\n",
    "\n",
    "X_combined_df = X_combined_df.drop(columns=datetime_columns)\n",
    "\n",
    "X_combined_df = X_combined_df.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    predictions = model.predict(X_combined_df)\n",
    "    submission = pd.DataFrame({\"id\": final_test.index, \"log_bike_count\": predictions.flatten()})\n",
    "    submission_path = f\"submission_{name}.csv\"\n",
    "    submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14266/14266 - 41s - 3ms/step - loss: 3.4890\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 5: Train the Model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 6: Evaluate the Model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_split, y_test_split)\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:217\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m ):\n\u001b[0;32m    216\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nishu\\anaconda3\\envs\\pds\\lib\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[1;34m(optional, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_split = X_train_split.values.reshape(X_train_split.shape[0], 1, -1)\n",
    "X_test_split = X_test_split.values.reshape(X_test_split.shape[0], 1, -1)\n",
    "\n",
    "# Step 4: Define LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation=\"relu\", input_shape=(X_train_split.shape[1], X_train_split.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "model.fit(X_train_split, y_train_split, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "loss = model.evaluate(X_test_split, y_test_split)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
